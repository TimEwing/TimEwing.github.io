<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="https://tim.fish/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tim.fish/" rel="alternate" type="text/html" /><updated>2023-12-27T21:21:00-07:00</updated><id>https://tim.fish/feed.xml</id><title type="html">tim.fish</title><subtitle>tim.fish is the personal website of Tim Ewing, a software engineer from  Boulder, Colorado.</subtitle><author><name>Tim Ewing</name></author><entry><title type="html">Southwest Research Institute</title><link href="https://tim.fish/2022/06/03/swri.html" rel="alternate" type="text/html" title="Southwest Research Institute" /><published>2022-06-03T12:23:00-06:00</published><updated>2022-06-03T12:23:00-06:00</updated><id>https://tim.fish/2022/06/03/swri</id><content type="html" xml:base="https://tim.fish/2022/06/03/swri.html">&lt;h1 id=&quot;swri&quot;&gt;SwRI&lt;/h1&gt;
&lt;p&gt;During my undergraduate degree, I worked part-time at Southwest Research Institure (SwRI) in Boulder, Colorado. The majority of my time was spent on two missions: CYGNSS, a constellation of eight satellites in low Earth orbit, and LUCY, a Discovery class mission on its way to the Trojan asteroids.&lt;/p&gt;

&lt;h1 id=&quot;cygnss&quot;&gt;CYGNSS&lt;/h1&gt;
&lt;p&gt;The Cyclone Global Navigation Satellite System (CYGNSS) is a constellation of eight approximately meter sized satellites in low Earth orbit. They use a Delay-Doppler Mapping Instrument (DDMI) to map ocean surface wind - that is, they listen to the reflection of GPS signals from the ocean surface and measure the amount of scatter caused by choppy waves to infer the surface wind speed. The data is used primarily to aid hurricane prediction efforts.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/cygnss_data.jpg&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      An example of CYGNSS DDMI data, from &lt;a href=&quot;https://www.nasa.gov/feature/nasa-s-cygnss-satellite-constellation-enters-science-operations-phase/&quot;&gt;nasa.gov&lt;/a&gt;

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;I was initially hired to help take over maintenance of the mission planning system, a Django-based web service which mission planners used to scheudle operations of the satellites. I also worked as a Flight Controller on the mission, meaning I was the last human in the line between the commands scheduled by the mission planners and the satellites themselves.&lt;/p&gt;

&lt;p&gt;After about a year at SwRI, I was also tasked with writing a few scripts for the mission’s &lt;a href=&quot;https://itos.gsfc.nasa.gov/index.php&quot;&gt;Integrated Test and Operations System&lt;/a&gt; which manages the playback of data during the satellites’ ground station overpasses. Scripts I wrote focused especially on data validation and playback; when blocks of data were found to be missing or corrupted after processing, my scripts would generate replay commands and pass them back to ITOS for replay on the next pass. These replays used extra time at the end of each pass in order to eliminate the need for extra planned replay passes. These scripts helped CYGNSS double the frequency of ground passes and reduce the latency between collection and use of data.&lt;/p&gt;

&lt;p&gt;I also authored and co-authored papers while at SwRI, both of which presented at the IEEE Aerospace Conference. The first paper, &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8741926&quot;&gt;&lt;em&gt;When You Have More Satellites than People: The Evolution of CYGNSS Flight Operations&lt;/em&gt;&lt;/a&gt; details the process we use to manage the constellation with minimal staff and how we use extensive automation in our process. The second paper, &lt;a href=&quot;https://ieeexplore.ieee.org/document/9438383&quot;&gt;&lt;em&gt;Django as a Mission Planning Tool Interface for the CYGNSS Mission&lt;/em&gt;&lt;/a&gt;, is a dive into the types of interfaces generally used for mission planning tools and a description of how web development strategies can be used to quickly iterate and test tools for mission planning. A full PDF is available &lt;a href=&quot;/assets/cycygnss_paper.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;embed src=&quot;/assets/cycygnss_paper.pdf&quot; type=&quot;application/pdf&quot; /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;lucy&quot;&gt;LUCY&lt;/h1&gt;
&lt;p&gt;LUCY is a Discovery class mission launched on October 16, 2021. It will fly by a series of Trojan asteroids, objects which orbit out of phase in the same orbit as Jupiter, using a complicated series of gravity assists.&lt;/p&gt;

&lt;figure&gt;
  &lt;video align=&quot;center&quot; width=&quot;100%&quot; autoplay=&quot;true&quot; loop=&quot;&quot; muted=&quot;&quot;&gt;
    &lt;source src=&quot;https://svs.gsfc.nasa.gov/vis/a000000/a004700/a004719/Lucy_full_01_1080p30.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
  &lt;figcaption&gt;LUCY&apos;s planned trajectory, via &lt;a hrev=&quot;https://svs.gsfc.nasa.gov/4719&quot;&gt;NASA&apos;s SVI&lt;/a&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;My role on LUCY was very similar to the work I did on CYGNSS; I helped design a mission planning interface using Python/Django from the ground up using lessons learned from the previous version. I built the initial versions of the codebase and helped build out both the backend and frontend. I also contributed to &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9438187&quot;&gt;&lt;em&gt;Lucy Science Planning: Incorporating Lessons Learned from over a Decade of Space Ops Experience&lt;/em&gt;&lt;/a&gt;, a paper presented at the 2021 IEEE Aerospace Conference.&lt;/p&gt;</content><author><name>Tim Ewing</name></author><summary type="html">Located in Boulder, Colorado where I worked on LUCY, a NASA Discovery class Mission and CYGNSS, a NASA Pathfinder class Mission.</summary></entry><entry><title type="html">omnichrome.py█</title><link href="https://tim.fish/2022/05/23/omnichrome.html" rel="alternate" type="text/html" title="omnichrome.py█" /><published>2022-05-23T15:59:42-06:00</published><updated>2022-05-23T15:59:42-06:00</updated><id>https://tim.fish/2022/05/23/omnichrome</id><content type="html" xml:base="https://tim.fish/2022/05/23/omnichrome.html">&lt;p&gt;&lt;a href=&quot;https://github.com/TimEwing/omni&quot;&gt;
  &lt;i class=&quot;fab fa-github&quot;&gt; View source on GitHub&lt;/i&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Omnichrome is a project I’ve been working on for almost as long as I’ve been able to code. I use it as a practice project to make sure I stay in form when I’m not doing much coding at work or school.&lt;/p&gt;

&lt;p&gt;What does it do?&lt;/p&gt;

&lt;p&gt;Omnichrome makes images.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/omni/splut.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;It makes pretty, colorful, fractal-esque images.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/omni/splat.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;Every pixel in the images is unique; every possible 7-bit color is there exactly once.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/omni/splurt.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;These images are produced by the current iteration, which is 5th or 6th total version of the program.&lt;/p&gt;

&lt;p&gt;The original inspiration was a post on &lt;a href=&quot;(https://codegolf.stackexchange.com/questions/22144/images-with-all-colors)&quot;&gt;codegolf.stackexchange&lt;/a&gt; that challenged competitors to create images with every color. They link to &lt;a href=&quot;allrgb.com&quot;&gt;allrgb.com&lt;/a&gt;; who knows where the original idea came from.&lt;/p&gt;

&lt;p&gt;I wrote my first version in middle school. I used a language called &lt;a href=&quot;https://processing.org/&quot;&gt;processing&lt;/a&gt;, which is really just Java with a nice graphics interface. I only have one sample that survived from that version:&lt;/p&gt;

&lt;figure style=&quot;width: 256px&quot; class=&quot;align-center&quot;&gt;
  &lt;img src=&quot;/assets/images/omni/flower.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Honestly, I still think it kinda looks good but realistically there’s no way it’s actually omnichromatic (meaning a unique and spanning set of pixels).&lt;/p&gt;

&lt;p&gt;The newest version is a lot more complex than that original version. In a sentence, it uses a K-D tree to iteratively place the nearest unused pixel in color to the neighbors moving outward from the center. At a high level that isn’t too complicated, but the details get pretty messy pretty quickly.&lt;/p&gt;

&lt;h1 id=&quot;how-to-build-an-omnichrome&quot;&gt;How to build an Omnichrome&lt;/h1&gt;

&lt;h2 id=&quot;step-one-make-a-stochastic-3d-voronoi-partition-of-the-entire-colorspace&quot;&gt;Step One: Make a stochastic 3D Voronoi partition of the entire colorspace&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Stochastic 3D Voronoi Partition&lt;/em&gt;. What?&lt;/p&gt;

&lt;p&gt;A Voronoi pattern is made by selecting a bunch of points on the plane, then coloring the plane based on the nearest point. Wikipedia has a nice example:&lt;/p&gt;

&lt;figure style=&quot;width: 350px&quot; class=&quot;align-center&quot;&gt;
  &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot;&gt;
    &lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/Euclidean_Voronoi_diagram.svg/1024px-Euclidean_Voronoi_diagram.svg.png&quot; /&gt;
  &lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;We’re doing the same thing, but we’re grouping &lt;em&gt;colorspace&lt;/em&gt; instead of normal euclidian space. However, since humans have three differently colored cones in our eyes, we need three dimensions in our colorspace, so we get more of a Voronoi Cube than a Voronoi Diagram.&lt;/p&gt;

&lt;p&gt;That’s not so hard. We’re randomly (&lt;em&gt;stochastically&lt;/em&gt;) selecting a bunch of colors, and grouping the rest of the &lt;em&gt;colorspace&lt;/em&gt; by which color is closest (&lt;em&gt;Voronoi partitioning&lt;/em&gt;). &lt;em&gt;Stochastic 3D Voronoi Partitioning&lt;/em&gt;. Neat.&lt;/p&gt;

&lt;h1 id=&quot;step-two-the-k-d-tree&quot;&gt;Step Two: The K-D Tree&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Use the Voronoi Partitions to build a K-D Tree&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;https://en.wikipedia.org/wiki/K-d_tree&quot;&gt;K-D Tree&lt;/a&gt; is a binary tree used to partition &lt;em&gt;spacial&lt;/em&gt; data instead of just numerical data. It works in some K number of dimensions instead of just one; a binary tree is a special case of a K-D tree where K=1. Normally, a K-D tree works somewhat differently than in our case. The Voronoi partioning gives much more asthetically pleasing results, but it isn’t a part of the normal process.&lt;/p&gt;

&lt;p&gt;For our case, we just use each group from the previous step (the Voronoi partitions) as a single branch of the tree. A normal K-D Tree has only two branches per step - we can have any number (although only one branch would be a pretty sad tree).&lt;/p&gt;

&lt;p&gt;The ‘key’ for each branch is just the point that corresponded to that group. Later, when we want to search through the tree, we just compare the color we’re searching to each branch’s initial color.&lt;/p&gt;

&lt;p&gt;Now, we just need to iterate the process: each sub-group from the original Voronoi partition is broken into a pattern of even more Voronoi partitions. Continue until each partition only has one pixel.&lt;/p&gt;

&lt;p&gt;Efficiently partitioning the whole space is a can of worms I don’t want to get into right now, but a push in the right direction would be to imagine each selected color as a heavy mass on a rubber sheet, and to imagine the other colors in the space as particles sliding down towards that color.&lt;/p&gt;

&lt;h1 id=&quot;step-three-search-the-tree&quot;&gt;Step Three: Search the Tree&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Starting in the middle of the image, place each pixel from the tree&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This part is easier. Since we already have the tree, it’s now pretty easy to search it for colors. All we have to do is this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pick a pixel to place; start at the middle, and spiral outward to the edges.&lt;/li&gt;
  &lt;li&gt;Average the neighboring pixels; search the K-D Tree for the nearest color to this average.&lt;/li&gt;
  &lt;li&gt;Pop that color out of the tree; place it at this pixel.&lt;/li&gt;
  &lt;li&gt;Return to step 1 until we’re out of pixels.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And that’s it. Tadaa! You’ve made an omnichromic (&lt;em&gt;omnichromatic?&lt;/em&gt;) picture!&lt;/p&gt;

&lt;h1 id=&quot;step-four-tune-the-result&quot;&gt;Step Four: Tune the result&lt;/h1&gt;

&lt;p&gt;Lots of parameters can be changed and tuned here to make better looking outputs. For example, when making the Voronoi partitions, there’s not really a good reason to use the normal Euclidian metric. I found the best results come from using a skewed metric which uses Hue, Saturation, and Value as the basis, treats Hue as circular, and weights distances in Hue stronger when Saturation and Value are higher.&lt;/p&gt;

&lt;p&gt;There’s also no reason you have to start in the middle and spiral out:&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/omni/drip.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;I personally think the circular ones are best, but to each their own. Someday I’ll probably write another version, but this is the project as it stands.&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;</content><author><name>Tim Ewing</name></author><summary type="html">Programmatically produced polychromatic pictures with picturesquely placed pixels</summary></entry><entry><title type="html">kepler.py█</title><link href="https://tim.fish/2022/05/21/kepler.html" rel="alternate" type="text/html" title="kepler.py█" /><published>2022-05-21T10:00:00-06:00</published><updated>2022-05-21T10:00:00-06:00</updated><id>https://tim.fish/2022/05/21/kepler</id><content type="html" xml:base="https://tim.fish/2022/05/21/kepler.html">&lt;p&gt;&lt;a href=&quot;https://github.com/TimEwing/kepler&quot;&gt;
  &lt;i class=&quot;fab fa-github&quot;&gt; View source on GitHub&lt;/i&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Kepler is a program which simulates what an Earth-based telescope would see when observing the transit of an exoplanet in front of another solar system’s star.&lt;/p&gt;

&lt;figure style=&quot;width: 218px&quot; class=&quot;align-center&quot;&gt;
  &lt;a href=&quot;https://xkcd.com/1371/&quot;&gt;
    &lt;img src=&quot;https://imgs.xkcd.com/comics/brightness.png&quot; /&gt;
  &lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;It was designed to generate short, educational animations to help explain the concept to students. I wrote it as an optional, extra-credit assignment for my Introduction to Astronomy class - the assignment was to create an educational graphic for future classes.&lt;/p&gt;

&lt;p&gt;Here’s a what Jupiter orbiting the Sun in Mercury’s orbit would look like:&lt;/p&gt;

&lt;video align=&quot;center&quot; width=&quot;100%&quot; autoplay=&quot;true&quot; loop=&quot;&quot; muted=&quot;&quot;&gt;
  &lt;source src=&quot;/assets/videos/kepler.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;
&lt;p&gt;The program actually generates four separate, synchronized animations:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Upper-left: The (exaggerated) color of what each pixel would see&lt;/li&gt;
  &lt;li&gt;Upper-right: The orbit, sun, and planet (to scale)&lt;/li&gt;
  &lt;li&gt;Center-right: The non-pixelated image of the planet crossing the star&lt;/li&gt;
  &lt;li&gt;Bottom: The lightcurve of the observation, with simulated noise.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The program is able to handle all the normal parameters for keplerian orbits. Here’s Jupiter on a completely unrealistic, inclined, comet-like path in front of the Sun:&lt;/p&gt;

&lt;video align=&quot;center&quot; width=&quot;100%&quot; autoplay=&quot;true&quot; loop=&quot;&quot; muted=&quot;&quot;&gt;
  &lt;source src=&quot;/assets/videos/kepler2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;You can also use Kepler to simulate lunar eclipses since it is essentially the same phenomenon (a big dark thing going in front of the big circular light thing):&lt;/p&gt;
&lt;video align=&quot;center&quot; width=&quot;100%&quot; autoplay=&quot;true&quot; loop=&quot;&quot; muted=&quot;&quot;&gt;
  &lt;source src=&quot;/assets/videos/kepler3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;I wrote the program in python and used basically only numpy and matplotlib; no libraries were used for orbital or geometry calculations. There’s a ton of 2-d and 3-d trig involved, and it got pretty nasty pretty quickly. It also includes a hand-built Newton’s Method minimum finder since I got sick of trying to analytically solve for the transit point.&lt;/p&gt;

&lt;p&gt;I’m not going to write up the mess that is the geometry involved here, but if you want a taste the code is at &lt;a href=&quot;https://github.com/TimEwing/kepler&quot;&gt;https://github.com/TimEwing/kepler&lt;/a&gt;. There is only one commit, no documentation, and only a few comments but other than the math its mostly just configuring matplotlib. The whole project was less than 48 hours start-to-finish. The highlight is probably the solution for the circle-circle-square intersection; that chunk of code took the majority of the project’s time.&lt;/p&gt;</content><author><name>Tim Ewing</name></author><summary type="html">Hand-made keplerian orbit simulation, made as extra credit for an introductory astronomy class.</summary></entry><entry><title type="html">nh_convert.sh█</title><link href="https://tim.fish/2022/05/18/nh-convert.html" rel="alternate" type="text/html" title="nh_convert.sh█" /><published>2022-05-18T10:00:00-06:00</published><updated>2022-05-18T10:00:00-06:00</updated><id>https://tim.fish/2022/05/18/nh-convert</id><content type="html" xml:base="https://tim.fish/2022/05/18/nh-convert.html">&lt;p&gt;&lt;a href=&quot;https://github.com/TimEwing/pluto&quot;&gt;
  &lt;i class=&quot;fab fa-github&quot;&gt; View source on GitHub&lt;/i&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One of the perks of interning at Southwest Research Institute was being able to do an independent study my senior year. I was asked to convert unresolved observations of Pluto from NASA’s New Horizons between two sets of optical filters. Since the bandwidth profile of the two filter sets didn’t correspond cleanly, conversion was fairly involved.&lt;/p&gt;

&lt;p&gt;A writeup of the project is available as a pdf &lt;a href=&quot;/assets/nh_conversion_tim_ewing.pdf&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;embed src=&quot;/assets/nh_conversion_tim_ewing.pdf&quot; type=&quot;application/pdf&quot; /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The code is structured as a Python data pipeline, segmented into many steps which were individually tested and verified. It’s available on &lt;a href=&quot;git@github.com:TimEwing/pluto.git&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;</content><author><name>Tim Ewing</name></author><summary type="html">New Horizons unresolved observation filter conversion, performed as an independent study.</summary></entry></feed>